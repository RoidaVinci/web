<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Gradient Backpropagation</title>
  <script src="https://cdn.jsdelivr.net/gh/jquery/jquery@3.6.1/dist/jquery.slim.min.js" integrity="sha256-w8CvhFs7iHNVUtnSP0YKEg00p9Ih13rlL9zGqvLdePA=" crossorigin="anonymous"></script>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="stylesheet" href="../assets/cmun-serif/cmun-serif.css">
  <link rel="stylesheet" href="../assets/article.css">
  <link rel="icon" type="image/x-icon" href="../favicon.ico">
  <link rel="shortcut icon" type="image/png" href="../favicon.png">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "AMScd.js"],
        equationNumbers: { autoNumber: "AMS" },
        Macros: {
          RR: "{\\mathbb{R}}",
          bold: ["{\\bf #1}", 1]
        }
      },
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      MathJax.typesetPromise();
    });
  </script>
</head>
<body>
  <!-- Back Button -->
  <a href="javascript:history.back()" class="back-button">
    <i class="fas fa-arrow-left"></i>
  </a>

  <main role="main" class="container-sm" style="max-width: 1080px">
    <h1 class="page-title">Gradient Backpropagation</h1>
    
    <h2 class="section-title">Introduction</h2>
    <p>
      In the article <a href="../articles/nn_graph.html">Neural Networks as Graphs</a> we explored how neural networks can be represented as graphs, with nodes corresponding to neurons and edges representing synaptic connections. One of the most crucial algorithms in the training of these networks is gradient descent, a fundamental tool in mathematical optimization. Despite the theoretical assumptions often not holding in typical neural network settings, gradient descent remains remarkably effective for training (i.e., optimizing the weights of) neural networks.
    </p>
    <p>
      This article delves into concepts that are part of the broader field of automatic differentiation, which serves as the theoretical foundation for popular deep learning frameworks like PyTorch and TensorFlow. We will first present the problem and its solution in a general context and then derive the well-known equations for simple feed-forward neural networks. This generality is not merely academic; it is essential when dealing with complex models like Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformers, as well as when handling trainable parameters in Layer or Batch Normalization, or connecting weights between distant layers as in ResNets or other deep networks.
    </p>

    <h2 class="section-title">Gradient Calculation and the Chain Rule</h2>
    <p>
      The chain rule is a fundamental tool that allows us to compute gradients efficiently, even in complex network structures. By applying the chain rule, it is possible to calculate:
    </p>

    <p>
      $$ \frac{\partial \mathcal{R}_\mathcal{D}}{\partial W_{g_0}} = \frac{\partial \mathcal{R}_\mathcal{D}}{\partial \mathcal{RN}} \frac{\partial \mathcal{RN}}{\partial a_{g_0}} \frac{\partial a_{g_0}}{\partial W_{g_0}} $$ 
    </p>

    <p>
      Here, \( W_{g_0} \) represents a parameter of the function \( f_{g_0} \) that is being updated, and \( \mathcal{RN} = (a_{g_s})_{g_s \in \mathcal{N}_s} \) is the output of the network. The partial derivatives of an output \( a_{g_s} \) with respect to a particular activation \( a_{g_0} \) can be expressed as:
    </p>

    <p>
      $$ \frac{\partial a_{g_s}}{\partial a_{g_0}} = \sum_{(g_0, g_1, \ldots, g_s) \in \mathcal{PATH}(g_0, g_s)} \prod_{j=1}^{s} \frac{\partial a_{g_{j}}}{\partial a_{g_{j-1}}} $$
    </p>

    <p>
      where \( \mathcal{PATH}(g_0, g_s) \) denotes the set of all paths in the graph \( (\mathcal{N}, \mathcal{A}) \) between nodes \( g_0 \) and \( g_s \).
    </p>

    <h2 class="section-title">Scalability and Efficiency</h2>
    <p>
      Directly applying the formula above, the number of gradients to be calculated at each node grows rapidly with the length of the paths. For instance, in a fully connected multilayer perceptron with \( L \) layers and uniform width \( (n, \ldots, n) \), computing a gradient of the output with respect to an activation at layer \( l \) would require calculating \( n^{L-l} \) partial derivatives. Therefore, calculating all partial derivatives of the network with respect to the inputs would require:
    </p>

    <p>
      $$ \sum_{l=0}^{L-1} n^{L+1-l} = n^{L+1} \frac{1-\frac{1}{n^L}}{1-\frac{1}{n}} = \frac{n^{L+2} - n^2}{n-1} $$
    </p>

    <p>
      This number is exponential with the network's depth. However, by recognizing that each path in the previous equation can be decomposed into two smaller paths \( (g_0, g_1) \) and \( (g_1, \ldots, g_s) \), we can store and reuse partial derivatives efficiently. Specifically, after computing the partial derivatives \( \frac{\partial a_{g_s}}{\partial a_{g_1}} \) for all nodes \( g_1 \in s_{g_0} \), we can apply the following recurrence relation:
    </p>

    <p>
      $$ \frac{\partial a_{g_s}}{\partial a_{g_0}} = \sum_{g_1 \in s_{g_0}} \frac{\partial a_{g_s}}{\partial a_{g_1}} \frac{\partial a_{g_1}}{\partial a_{g_0}}. $$
    </p>

    <p>
      This reduces the complexity to the computation of \( n \) partial derivatives per layer in \( L \) layers, resulting in \( nL \) partial derivatives for the entire network. The trade-off is increased memory consumption, as the partial derivatives must be stored. Nonetheless, this provides a powerful algorithm for gradient backpropagation:
    </p>

    <p>
      $$ \frac{\partial \mathcal{R}_\mathcal{D}}{\partial W_{g_0}} = \sum_{g_1 \in s_{g_0}} \frac{\partial \mathcal{R}_\mathcal{D}}{\partial a_{g_1}} \frac{\partial a_{g_1}}{\partial a_{g_0}} \frac{\partial a_{g_0}}{\partial W_{g_0}}. $$
    </p>

    <h2 class="section-title">Matrix Form and Further Efficiency</h2>
    <p>
      In what follows, we focus on gradient calculations specifically for the multilayer perceptron. In addition to the benefits provided by the described recurrence, the affine nature of the perceptron's transformations allows us to further enhance efficiency by applying the chain rule in matrix form. This approach significantly accelerates computation, particularly in deep networks with many layers.
    </p>

    <p>
      Consider the earlier recurrence relation. By substituting and applying the chain rule in matrix notation, we arrive at:
    </p>

    <p>
      $$ \frac{\partial a^{(L)}_i}{\partial a^{(l)}_{j}} = \sum_{k=1}^{n_{l+1}} \frac{\partial a^{(L)}_i}{\partial a^{(l+1)}_k} \frac{\partial a^{(l+1)}_k}{\partial a^{(l)}_j} = \left(\frac{\partial a^{(L)}_i}{\partial a^{(l+1)}_k}\right)_{k=1}^{n_{l+1}} \cdot \left(\frac{\partial a^{(l)}_k}{\partial a^{(l)}_j}\right)_{k=1}^{n_{l+1}} = \left(\frac{\partial a^{(L)}}{\partial a^{(l+1)}}\right)^{\text{row} i} \cdot \left(\frac{\partial a^{(L)}}{\partial a^{(l+1)}}\right)^{\text{col} j}, $$
    </p>

    <p>
      which leads to the more general matrix expression:
    </p>

    <p>
      $$ \frac{\partial a^{(L)}}{\partial a^{(l)}} = \frac{\partial a^{(L)}}{\partial a^{(l+1)}} \frac{\partial a^{(l+1)}}{\partial a^{(l)}}. $$
    </p>

    <h2 class="section-title">Lemma and Recurrence Relations</h2>
    <div class="theorem">
      <div class="theorem-title">Lemma:</div>
      <p>In a multilayer perceptron, the following hold:</p>
      <p>
      $$ \frac{\partial z^{l+1}}{\partial a^{l}} = W^{(l+1)}, \ \ \frac{\partial a^{(l)}}{\partial z^{(l)}} = \nabla \text{act}_{l}(z^{(l)}), \ \ \frac{\partial z^{(l)}}{\partial W^{(l)}_{ij}} = a^{(l-1)}_j e_i, \ \ \frac{\partial z^{(l)}}{\partial b^{(l)}_{i}} = e_i. $$
      </p>
    </div>

    <p>
      With this understanding, we can now describe a complete and detailed algorithm for computing the partial derivative of the empirical risk with respect to any weight in the network, iteratively calculated backward through the network layers:
    </p>

    <div class="theorem">
      <div class="theorem-title">Proposition: Backpropagation algorithm for multilayer perceptrons</div>
      <p>Given a multilayer perceptron \( \mathcal{P} = (\mathcal{N}, \mathcal{A}, \mathcal{P}, \{ \text{act}_l \}_{l=1}^L, (W, b)) \), a training set \( \mathcal{D} = \{ (x_i, y_i) \}_{i=1}^N \), and a loss function \( L : \mathbb{R}^m \times \mathbb{R}^m \longrightarrow \mathbb{R} \), inducing an empirical risk \( \mathcal{R}_\mathcal{D} \). Let:</p>
      <p>
      $$ G^{(L)} = \frac{1}{N} \sum_{(x, y) \in \mathcal{D}} \partial_1 L(a^{(L)}(x), y) \ \nabla \text{act}_L(z^{(L)}(x)) \in \mathbb{R}^{(1, n_L)}, $$
      </p>
      <p>
      $$ G^{(l)} = \frac{1}{N} \sum_{(x, y) \in \mathcal{D}} G^{(l+1)} \ W^{(l+1)} \ \nabla \text{act}_l(z^{(l)}) \in \mathbb{R}^{(1, n_l)}. $$
      </p>
      <p>If each \( \text{act}_l \) is differentiable in a neighborhood of \( z^{(l)} \), then:</p>
      <p>
      $$ \frac{\partial \mathcal{R}_{\mathcal{D}}}{\partial W^{(l)}_{ij}}(W) = \frac{1}{N} \sum_{(x, y) \in \mathcal{D}} G^{(l)}_i a^{(l-1)}_j(x) \in \mathbb{R}, \ \ \ \frac{\partial \mathcal{R}_{\mathcal{D}}}{\partial b^{(l)}_i} = G^{(l)}_i \in \mathbb{R}. $$
      </p>
    </div>

    <h2 class="section-title">Conclusion</h2>
    <p>
      The backpropagation algorithm, as described, significantly reduces the computational complexity of training multilayer perceptrons by leveraging the chain rule and matrix calculus. By breaking down the gradients into manageable components and using recurrence relations, the algorithm ensures that gradients are computed efficiently and accurately, making it possible to train deep neural networks effectively.
    </p>
    <p>
      This general approach to backpropagation is not only applicable to simple feed-forward networks but also extends to more complex architectures such as recurrent neural networks, convolutional neural networks, and transformers. As deep learning models continue to evolve and grow in complexity, understanding the underlying principles of backpropagation remains essential for developing efficient and scalable machine learning solutions.
    </p>
    <p>
      To explore more about how neural networks can be conceptualized and analyzed as computational graphs, please refer to our previous article on <a href="../articles/nn_graph.html">Neural Networks as Graphs</a>.
    </p>

    <h2 class="section-title">References</h2>
    <p>
      <strong>[1]</strong> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536.
    </p>
    <p>
      <strong>[2]</strong> Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press.
    </p>
    <p>
      <strong>[3]</strong> LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.
    </p>
  </main>
</body>
</html>

